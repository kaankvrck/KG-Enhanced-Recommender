{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfddd0c-92ef-40c3-b593-be5f831f1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b06ca6a-7421-466f-bb4d-25ed6756f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703615c4-5b71-45b7-96a3-b574b3f614b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic path settings\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"dataset\")\n",
    "TRAIN_TEST_SPLIT_DIR = os.path.join(BASE_DIR, \"train_test_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc33813-daf2-44a1-901a-9c8686f0af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Maps\n",
    "with open(os.path.join(DATA_DIR, 'user_id_map.pkl'), 'rb') as f:\n",
    "    user_id_map = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'product_id_map.pkl'), 'rb') as f:\n",
    "    product_id_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3130606b-938c-4cdc-92c0-736bd1850100",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RecommenderSystem\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a170f4ef-6a2a-4fd7-b44d-4648eae0924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "train_sales_path = os.path.join(TRAIN_TEST_SPLIT_DIR, \"train_sales_data.csv\")\n",
    "test_sales_path = os.path.join(TRAIN_TEST_SPLIT_DIR, \"test_sales_data.csv\")\n",
    "train_df = pd.read_csv(train_sales_path)\n",
    "test_df = pd.read_csv(test_sales_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d266a6-13ff-45bc-8de5-af43f8989104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common user IDs: 1148\n",
      "Common product IDs: 1148\n"
     ]
    }
   ],
   "source": [
    "# Check common user and product IDs\n",
    "common_user_ids = set(train_df['user_id']).intersection(set(test_df['user_id']))\n",
    "common_product_ids = set(train_df['product_id']).intersection(set(test_df['product_id']))\n",
    "\n",
    "print(f\"Common user IDs: {len(common_user_ids)}\")\n",
    "print(f\"Common product IDs: {len(common_product_ids)}\")\n",
    "\n",
    "if len(common_user_ids) == 0 or len(common_product_ids) == 0:\n",
    "    raise ValueError(\"No common user IDs or product IDs between training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25c4cdb-8d82-4188-a63f-26ea04aa9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame:\n",
      "    user id                        product id Interaction type   \n",
      "0      3.0  2c55cae269aebf53838484b0d7dd931a             like  \\\n",
      "1      7.0  40d3cd16b41970ae6872e914aecf2c8e         purchase   \n",
      "2      8.0  bc178f33a04dbccefa95b165f8b56830             view   \n",
      "3      9.0  cc2083338a16c3fe2f7895289d2e98fe             like   \n",
      "4     14.0  82c86a4d24dce5e14303033d7b658b78             view   \n",
      "\n",
      "            Time stamp  Unnamed: 4  user_id  product_id  interaction_type  \n",
      "0  2023-10-12 08:00:00         NaN        2         517                 2  \n",
      "1  2023-10-16 08:00:00         NaN        6         759                 3  \n",
      "2  2023-10-17 08:00:00         NaN        7        2235                 1  \n",
      "3  2023-10-18 08:00:00         NaN        8        2405                 2  \n",
      "4  2023-10-23 08:00:00         NaN       13        1510                 1   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure there are data rows in test data\n",
    "if test_df.empty:\n",
    "    raise ValueError(\"Test DataFrame is empty.\")\n",
    "print(\"Test DataFrame:\\n\", test_df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d4c39e-c164-42c1-b20d-f76ff30e3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert interaction_type column to numeric\n",
    "test_df['interaction_type'] = pd.to_numeric(test_df['interaction_type'], errors='coerce')\n",
    "test_df.dropna(subset=['interaction_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e20bc84-bcb3-4d68-9c41-2328e1499fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataframe is still empty after conversion and dropping NA values\n",
    "if test_df.empty:\n",
    "    raise ValueError(\"Test DataFrame is empty after converting 'interaction_type' to numeric and dropping NA values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a089c1c-58f5-4c71-b291-cd970713ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to Spark DataFrame\n",
    "test_data = [Row(user_id=int(row.user_id), product_id=int(row.product_id), interaction_type=float(row['interaction_type'])) for index, row in test_df.iterrows()]\n",
    "test_df_spark = spark.createDataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c39e4c6-a61b-469a-bcb8-1923735ea5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame:\n",
      " Row(user_id=2, product_id=517, interaction_type=2.0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test DataFrame:\\n\", test_df_spark.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83ce016-4130-4f0e-a75a-311b7572778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model using Spark's load method\n",
    "model_path = os.path.join(BASE_DIR, \"als_model\")\n",
    "model = ALSModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e647c7e-e3a8-47d0-9b4a-9c17aebd5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on test data\n",
    "predictions = model.transform(test_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ddc2bce-2043-4dd5-984f-6655996f77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions.head(1) == []:\n",
    "    raise ValueError(\"Predictions DataFrame is empty after model.transform(). Ensure your test data has sufficient entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c217456e-d527-4d54-9f60-1e69efbfc326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+----------+\n",
      "|user_id|product_id|interaction_type|prediction|\n",
      "+-------+----------+----------------+----------+\n",
      "|      2|       517|             2.0| 1.9074912|\n",
      "|      6|       759|             3.0| 2.9219954|\n",
      "|      7|      2235|             1.0| 0.8998625|\n",
      "|      8|      2405|             2.0| 1.9074912|\n",
      "|     13|      1510|             1.0| 0.8998625|\n",
      "|     15|      2470|             2.0| 1.9074912|\n",
      "|     18|       885|             2.0| 1.9074913|\n",
      "|     20|      1990|             3.0| 2.9219954|\n",
      "|     23|      1545|             1.0| 0.8998625|\n",
      "|     25|      1263|             2.0| 1.9074911|\n",
      "|     26|       589|             3.0|  2.921995|\n",
      "|     28|      2980|             2.0| 1.9074911|\n",
      "|     33|       431|             1.0| 0.8998625|\n",
      "|     37|       854|             1.0| 0.8998625|\n",
      "|     39|      2305|             3.0| 2.9219952|\n",
      "|     41|      1912|             1.0| 0.8998625|\n",
      "|     48|       929|             2.0| 1.9074911|\n",
      "|     49|      1976|             3.0| 2.9219952|\n",
      "|     52|       911|             2.0| 1.9074911|\n",
      "|     53|      1350|             1.0|0.89986247|\n",
      "+-------+----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc364238-e736-476d-824d-f37c0e217498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.0911734855821343\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"interaction_type\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b727ebcc-a396-4b3a-b97a-c65791e84d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{1744, 1.4342929...|\n",
      "|      3|[{405, 1.3465422}...|\n",
      "|      5|[{2574, 2.063318}...|\n",
      "|      6|[{759, 2.9219956}...|\n",
      "|     12|[{117, 2.0395474}...|\n",
      "|     13|[{198, 1.5998125}...|\n",
      "|     16|[{2540, 2.9219954...|\n",
      "|     19|[{2976, 2.9219952...|\n",
      "|     20|[{1990, 2.9219954...|\n",
      "|     22|[{1796, 2.1820056...|\n",
      "|     26|[{589, 2.921995},...|\n",
      "|     27|[{2176, 1.4673496...|\n",
      "|     28|[{313, 2.0426118}...|\n",
      "|     31|[{1064, 1.5370151...|\n",
      "|     34|[{275, 2.3007212}...|\n",
      "|     40|[{723, 2.921995},...|\n",
      "|     47|[{1268, 1.625262}...|\n",
      "|     48|[{1197, 2.0176582...|\n",
      "|     52|[{1084, 2.075826}...|\n",
      "|     53|[{61, 1.4719193},...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting recommendations for users\n",
    "user_recs = model.recommendForAllUsers(10)\n",
    "user_recs.show()\n",
    "user_recs = model.recommendForAllUsers(100).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5070758e-2ac6-4a64-8f47-42ff9bcf8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d62f118-26ba-4a23-9c46-29213d93a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the Neo4j database\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"1234123412\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efe237-8b7d-4d7e-bf84-33be5f37edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Recommendations to Neo4j\n",
    "for index, row in user_recs.iterrows():\n",
    "    original_user_id = user_id_map[row['user_id']]\n",
    "    user_node = graph.nodes.match(\"Customer\", id=original_user_id).first()\n",
    "    recommendations = row['recommendations']\n",
    "    for rec in recommendations:\n",
    "        original_product_id = product_id_map[rec['product_id']]\n",
    "        score = rec['rating']\n",
    "        product_node = graph.nodes.match(\"Product\", id=original_product_id).first()\n",
    "        if user_node and product_node:\n",
    "            recommendation = Relationship(user_node, \"RECOMMENDED\", product_node, score=score)\n",
    "            graph.create(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a3510-398c-45d0-a746-62a3eaa58d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
